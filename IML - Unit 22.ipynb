{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split #, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Sajjad/08-2023/Python Code/Introduction to Machine Learning\n",
      "/mnt/d/Sajjad/08-2023/Python Code/Introduction to Machine Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#change working directory to the location of the data file\n",
    "os.chdir('/mnt/d/Sajjad/08-2023/Python Code/Introduction to Machine Learning/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data from UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 891, 'name': 'CDC Diabetes Health Indicators', 'repository_url': 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators', 'data_url': 'https://archive.ics.uci.edu/static/public/891/data.csv', 'abstract': 'The Diabetes Health Indicators Dataset contains healthcare statistics and lifestyle survey information about people in general along with their diagnosis of diabetes. The 35 features consist of some demographics, lab test results, and answers to survey questions for each patient. The target variable for classification is whether a patient has diabetes, is pre-diabetic, or healthy. ', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Tabular', 'Multivariate'], 'num_instances': 253680, 'num_features': 21, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Sex', 'Age', 'Education Level', 'Income'], 'target_col': ['Diabetes_binary'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2017, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C53919', 'creators': [], 'intro_paper': {'title': 'Incidence of End-Stage Renal Disease Attributed to Diabetes Among Persons with Diagnosed Diabetes — United States and Puerto Rico, 2000–2014', 'authors': 'Nilka Rios Burrows, MPH; Israel Hora, PhD; Linda S. Geiss, MA; Edward W. Gregg, PhD; Ann Albright, PhD', 'published_in': 'Morbidity and Mortality Weekly Report', 'year': 2017, 'url': 'https://www.cdc.gov/mmwr/volumes/66/wr/mm6643a2.htm', 'doi': None}, 'additional_info': {'summary': 'Dataset link: https://www.cdc.gov/brfss/annual_data/annual_2014.html', 'purpose': 'To better understand the relationship between  lifestyle and diabetes in the US', 'funded_by': 'The CDC', 'instances_represent': 'Each row represents a person participating in this study.', 'recommended_data_splits': 'Cross validation or a fixed train-test split could be used.', 'sensitive_data': '- Gender\\n- Income\\n- Education level', 'preprocessing_description': 'Bucketing of age', 'variable_info': '- Diabetes diagnosis\\n- Demographics (race, sex)\\n- Personal information (income, educations)\\n- Health history (drinking, smoking, mental health, physical health)', 'citation': None}, 'external_url': 'https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset'}\n",
      "                    name     role     type      demographic  \\\n",
      "0                     ID       ID  Integer             None   \n",
      "1        Diabetes_binary   Target   Binary             None   \n",
      "2                 HighBP  Feature   Binary             None   \n",
      "3               HighChol  Feature   Binary             None   \n",
      "4              CholCheck  Feature   Binary             None   \n",
      "5                    BMI  Feature  Integer             None   \n",
      "6                 Smoker  Feature   Binary             None   \n",
      "7                 Stroke  Feature   Binary             None   \n",
      "8   HeartDiseaseorAttack  Feature   Binary             None   \n",
      "9           PhysActivity  Feature   Binary             None   \n",
      "10                Fruits  Feature   Binary             None   \n",
      "11               Veggies  Feature   Binary             None   \n",
      "12     HvyAlcoholConsump  Feature   Binary             None   \n",
      "13         AnyHealthcare  Feature   Binary             None   \n",
      "14           NoDocbcCost  Feature   Binary             None   \n",
      "15               GenHlth  Feature  Integer             None   \n",
      "16              MentHlth  Feature  Integer             None   \n",
      "17              PhysHlth  Feature  Integer             None   \n",
      "18              DiffWalk  Feature   Binary             None   \n",
      "19                   Sex  Feature   Binary              Sex   \n",
      "20                   Age  Feature  Integer              Age   \n",
      "21             Education  Feature  Integer  Education Level   \n",
      "22                Income  Feature  Integer           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                          Patient ID  None             no  \n",
      "1         0 = no diabetes 1 = prediabetes or diabetes  None             no  \n",
      "2                          0 = no high BP 1 = high BP  None             no  \n",
      "3        0 = no high cholesterol 1 = high cholesterol  None             no  \n",
      "4   0 = no cholesterol check in 5 years 1 = yes ch...  None             no  \n",
      "5                                     Body Mass Index  None             no  \n",
      "6   Have you smoked at least 100 cigarettes in you...  None             no  \n",
      "7        (Ever told) you had a stroke. 0 = no 1 = yes  None             no  \n",
      "8   coronary heart disease (CHD) or myocardial inf...  None             no  \n",
      "9   physical activity in past 30 days - not includ...  None             no  \n",
      "10  Consume Fruit 1 or more times per day 0 = no 1...  None             no  \n",
      "11  Consume Vegetables 1 or more times per day 0 =...  None             no  \n",
      "12  Heavy drinkers (adult men having more than 14 ...  None             no  \n",
      "13  Have any kind of health care coverage, includi...  None             no  \n",
      "14  Was there a time in the past 12 months when yo...  None             no  \n",
      "15  Would you say that in general your health is: ...  None             no  \n",
      "16  Now thinking about your mental health, which i...  None             no  \n",
      "17  Now thinking about your physical health, which...  None             no  \n",
      "18  Do you have serious difficulty walking or clim...  None             no  \n",
      "19                                0 = female 1 = male  None             no  \n",
      "20  13-level age category (_AGEG5YR see codebook) ...  None             no  \n",
      "21  Education level (EDUCA see codebook) scale 1-6...  None             no  \n",
      "22  Income scale (INCOME2 see codebook) scale 1-8 ...  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features \n",
    "y = cdc_diabetes_health_indicators.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(cdc_diabetes_health_indicators.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(cdc_diabetes_health_indicators.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Education and Income are in ordinal scale, hence, no need to do one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Education Variable as Categorical\n",
    "#X['Education'] = X['Education'].astype('category')\n",
    "#X['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  HeartDiseaseorAttack  \\\n",
       "0       1         1          1   40       1       0                     0   \n",
       "1       0         0          0   25       1       0                     0   \n",
       "2       1         1          1   28       0       0                     0   \n",
       "3       1         0          1   27       0       0                     0   \n",
       "4       1         1          1   24       0       0                     0   \n",
       "\n",
       "   PhysActivity  Fruits  Veggies  ...  AnyHealthcare  NoDocbcCost  GenHlth  \\\n",
       "0             0       0        1  ...              1            0        5   \n",
       "1             1       0        0  ...              0            1        3   \n",
       "2             0       1        0  ...              1            1        5   \n",
       "3             1       1        1  ...              1            0        2   \n",
       "4             1       1        1  ...              1            0        2   \n",
       "\n",
       "   MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \n",
       "0        18        15         1    0    9          4       3  \n",
       "1         0         0         0    0    7          6       1  \n",
       "2        30        30         1    0    9          4       8  \n",
       "3         0         0         0    0   11          3       6  \n",
       "4         3         0         0    0   11          5       4  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making two train/test dataset. One without scaling and the other with scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without scaling\n",
    "trainX2, testX2, trainy2, testy2 = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "trainX, testX, trainy, testy = train_test_split(X_scaled, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes_binary\n",
      "0                  152917\n",
      "1                   24659\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Diabetes_binary\n",
       "0                  65417\n",
       "1                  10687\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the distribution of y in train and test\n",
    "print(trainy.value_counts())\n",
    "testy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177576, 21)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76104, 21)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this type casting is not always required but at times torch generates an error so just as a matter of caution converting all types to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all int64 to float32\n",
    "trainX = trainX.astype('float32')\n",
    "testX = testX.astype('float32')\n",
    "trainy = trainy.astype('float32')\n",
    "testy = testy.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using \"torch\" as keras_backend. Could have used tensorflow or jax as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore batch size, iteration size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_82 (Dense)            (None, 10)                220       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281 (1.10 KB)\n",
      "Trainable params: 281 (1.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = trainX.shape[1]\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 neurons and specify the input shape\n",
    "model.add(Dense(10, input_dim=n_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 5 neurons\n",
    "model.add(Dense(5, activation='relu'))\n",
    "\n",
    "# Add the output layer with 1 neuron (for binary classification) and 'sigmoid' activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 4s 8ms/step - loss: 0.4434 - auc: 0.6517\n",
      "Epoch 2/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3227 - auc: 0.8133\n",
      "Epoch 3/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3185 - auc: 0.8199\n",
      "Epoch 4/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3164 - auc: 0.8229\n",
      "Epoch 5/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3151 - auc: 0.8248\n",
      "Epoch 6/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3143 - auc: 0.8261\n",
      "Epoch 7/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3137 - auc: 0.8270\n",
      "Epoch 8/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3133 - auc: 0.8276\n",
      "Epoch 9/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3130 - auc: 0.8280\n",
      "Epoch 10/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3127 - auc: 0.8284\n",
      "Epoch 11/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3125 - auc: 0.8286\n",
      "Epoch 12/20\n",
      "347/347 [==============================] - 3s 9ms/step - loss: 0.3123 - auc: 0.8288\n",
      "Epoch 13/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3122 - auc: 0.8290\n",
      "Epoch 14/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3121 - auc: 0.8292\n",
      "Epoch 15/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3120 - auc: 0.8293\n",
      "Epoch 16/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3119 - auc: 0.8294\n",
      "Epoch 17/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3118 - auc: 0.8296\n",
      "Epoch 18/20\n",
      "347/347 [==============================] - 3s 9ms/step - loss: 0.3118 - auc: 0.8296\n",
      "Epoch 19/20\n",
      "347/347 [==============================] - 3s 9ms/step - loss: 0.3116 - auc: 0.8299\n",
      "Epoch 20/20\n",
      "347/347 [==============================] - 3s 8ms/step - loss: 0.3116 - auc: 0.8299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f66bbc493c0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainy, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 10ms/step - loss: 0.3147 - auc: 0.8287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3146805167198181, 0.8286526799201965]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX, testy, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing the performance with catboost, xgboost, and lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_models = 100\n",
    "depth_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 24659, number of negative: 152917\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 202\n",
      "[LightGBM] [Info] Number of data points in the train set: 177576, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.138864 -> initscore=-1.824753\n",
      "[LightGBM] [Info] Start training from score -1.824753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LG Boost  :  0.8296622740763209\n",
      "Total time LGB:  0.46257662773132324\n"
     ]
    }
   ],
   "source": [
    "#use lgboost\n",
    "lgb_model = lgb.LGBMClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "lgb_model.fit(trainX2,trainy2)\n",
    "md_probs = lgb_model.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"LG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time LGB: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat Boost  :  0.8289070111234721\n",
      "Total time CB:  1.9635086059570312\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(iterations=num_of_models, depth=depth_level, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "#record the start time\n",
    "start_time = time.time()\n",
    "cb.fit(trainX2,trainy2)\n",
    "md_probs = cb.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"Cat Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time CB: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Boost  :  0.8295503663157561\n",
      "Total time XGB:  1.6627285480499268\n"
     ]
    }
   ],
   "source": [
    "#use xgboost\n",
    "xgb_model = xgb.XGBClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "xgb_model.fit(trainX2,trainy2)\n",
    "md_probs = xgb_model.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"XG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time XGB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using sklearn version of feedforward neural network (also called Multi-layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35431792\n",
      "Epoch 1, AUC ROC: 0.8236\n",
      "Iteration 1, loss = 0.31578031\n",
      "Epoch 2, AUC ROC: 0.8266\n",
      "Iteration 1, loss = 0.31479602\n",
      "Epoch 3, AUC ROC: 0.8269\n",
      "Iteration 1, loss = 0.31466764\n",
      "Epoch 4, AUC ROC: 0.8270\n",
      "Iteration 1, loss = 0.31458664\n",
      "Epoch 5, AUC ROC: 0.8268\n",
      "Iteration 1, loss = 0.31459864\n",
      "Epoch 6, AUC ROC: 0.8272\n",
      "Iteration 1, loss = 0.31455789\n",
      "Epoch 7, AUC ROC: 0.8274\n",
      "Iteration 1, loss = 0.31444263\n",
      "Epoch 8, AUC ROC: 0.8276\n",
      "Iteration 1, loss = 0.31438872\n",
      "Epoch 9, AUC ROC: 0.8272\n",
      "Iteration 1, loss = 0.31453122\n",
      "Epoch 10, AUC ROC: 0.8279\n"
     ]
    }
   ],
   "source": [
    "#using multilayer perceptron of sklearn\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=10, solver='adam', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1, batch_size=2048)\n",
    "#mlp.fit(trainX, trainy)\n",
    "for epoch in range(10):  # Set the desired number of epochs\n",
    "    mlp.partial_fit(trainX, trainy, classes=[0, 1])\n",
    "\n",
    "    # Evaluate on the validation set and print AUC ROC\n",
    "    y_prob = mlp.predict_proba(testX)[:, 1]\n",
    "    auc_roc = roc_auc_score(testy, y_prob)\n",
    "    print(f\"Epoch {epoch + 1}, AUC ROC: {auc_roc:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3838 - auc: 0.6940\n",
      "Epoch 2/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3300 - auc: 0.7994\n",
      "Epoch 3/20\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3253 - auc: 0.8078\n",
      "Epoch 4/20\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3230 - auc: 0.8115\n",
      "Epoch 5/20\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3215 - auc: 0.8140\n",
      "Epoch 6/20\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3204 - auc: 0.8158\n",
      "Epoch 7/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3196 - auc: 0.8173\n",
      "Epoch 8/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3189 - auc: 0.8185\n",
      "Epoch 9/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3182 - auc: 0.8194\n",
      "Epoch 10/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3177 - auc: 0.8203\n",
      "Epoch 11/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3172 - auc: 0.8211\n",
      "Epoch 12/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3168 - auc: 0.8217\n",
      "Epoch 13/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3164 - auc: 0.8222\n",
      "Epoch 14/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3161 - auc: 0.8228\n",
      "Epoch 15/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3158 - auc: 0.8232\n",
      "Epoch 16/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3155 - auc: 0.8237\n",
      "Epoch 17/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3153 - auc: 0.8240\n",
      "Epoch 18/20\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3151 - auc: 0.8244\n",
      "Epoch 19/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3149 - auc: 0.8247\n",
      "Epoch 20/20\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3147 - auc: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f65ad0aee30>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = trainX.shape[1]\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 neurons and specify the input shape\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 5 neurons\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer with 1 neuron (for binary classification) and 'sigmoid' activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(trainX, trainy, epochs=20, batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3177793323993683, 0.8239396810531616]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX, testy, verbose=0, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with early stopping without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3145 - auc: 0.8253\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3144 - auc: 0.8255\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3142 - auc: 0.8257\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3141 - auc: 0.8260\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3140 - auc: 0.8262\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3138 - auc: 0.8263\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3137 - auc: 0.8265\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3136 - auc: 0.8267\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3135 - auc: 0.8268\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3134 - auc: 0.8270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f65acfd6890>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define EarlyStopping callback to monitor training loss\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with dropout and early stopping\n",
    "model.fit(trainX, trainy, epochs=10, batch_size=2048,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testX, testy, verbose=0, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 0.5168 - auc: 0.5813\n",
      "Epoch 2/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3839 - auc: 0.7148\n",
      "Epoch 3/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3639 - auc: 0.7530\n",
      "Epoch 4/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3544 - auc: 0.7693\n",
      "Epoch 5/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3493 - auc: 0.7764\n",
      "Epoch 6/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3465 - auc: 0.7806\n",
      "Epoch 7/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3432 - auc: 0.7862\n",
      "Epoch 8/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3412 - auc: 0.7887\n",
      "Epoch 9/30\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3390 - auc: 0.7932\n",
      "Epoch 10/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3384 - auc: 0.7936\n",
      "Epoch 11/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3375 - auc: 0.7953\n",
      "Epoch 12/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3363 - auc: 0.7972\n",
      "Epoch 13/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3347 - auc: 0.7995\n",
      "Epoch 14/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3333 - auc: 0.8012\n",
      "Epoch 15/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3326 - auc: 0.8018\n",
      "Epoch 16/30\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3324 - auc: 0.8023\n",
      "Epoch 17/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3316 - auc: 0.8038\n",
      "Epoch 18/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3306 - auc: 0.8051\n",
      "Epoch 19/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3299 - auc: 0.8057\n",
      "Epoch 20/30\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3297 - auc: 0.8062\n",
      "Epoch 21/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3288 - auc: 0.8075\n",
      "Epoch 22/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3288 - auc: 0.8077\n",
      "Epoch 23/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3281 - auc: 0.8087\n",
      "Epoch 24/30\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3280 - auc: 0.8086\n",
      "Epoch 25/30\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3273 - auc: 0.8099\n",
      "Epoch 26/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3275 - auc: 0.8100\n",
      "Epoch 27/30\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3266 - auc: 0.8111\n",
      "Epoch 28/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3267 - auc: 0.8111\n",
      "Epoch 29/30\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3261 - auc: 0.8120\n",
      "Epoch 30/30\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3248 - auc: 0.8138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f65ace3ae30>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.3\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.3\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=30, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6265 - auc: 0.5015\n",
      "Epoch 2/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.4080 - auc: 0.6938\n",
      "Epoch 3/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3564 - auc: 0.7626\n",
      "Epoch 4/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3431 - auc: 0.7820\n",
      "Epoch 5/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3370 - auc: 0.7918\n",
      "Epoch 6/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3336 - auc: 0.7976\n",
      "Epoch 7/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3315 - auc: 0.8012\n",
      "Epoch 8/150\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3287 - auc: 0.8053\n",
      "Epoch 9/150\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3272 - auc: 0.8078\n",
      "Epoch 10/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3260 - auc: 0.8093\n",
      "Epoch 11/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3251 - auc: 0.8108\n",
      "Epoch 12/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3235 - auc: 0.8134\n",
      "Epoch 13/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3236 - auc: 0.8132\n",
      "Epoch 14/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3227 - auc: 0.8143\n",
      "Epoch 15/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3217 - auc: 0.8160\n",
      "Epoch 16/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3219 - auc: 0.8155\n",
      "Epoch 17/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3213 - auc: 0.8168\n",
      "Epoch 18/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3211 - auc: 0.8169\n",
      "Epoch 19/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3205 - auc: 0.8177\n",
      "Epoch 20/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3200 - auc: 0.8186\n",
      "Epoch 21/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3196 - auc: 0.8187\n",
      "Epoch 22/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3193 - auc: 0.8195\n",
      "Epoch 23/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3193 - auc: 0.8195\n",
      "Epoch 24/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3188 - auc: 0.8201\n",
      "Epoch 25/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3187 - auc: 0.8200\n",
      "Epoch 26/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3186 - auc: 0.8203\n",
      "Epoch 27/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3182 - auc: 0.8207\n",
      "Epoch 28/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3180 - auc: 0.8211\n",
      "Epoch 29/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3177 - auc: 0.8216\n",
      "Epoch 30/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3179 - auc: 0.8213\n",
      "Epoch 31/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3176 - auc: 0.8218\n",
      "Epoch 32/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3173 - auc: 0.8222\n",
      "Epoch 33/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3168 - auc: 0.8226\n",
      "Epoch 34/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3170 - auc: 0.8226\n",
      "Epoch 35/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3175 - auc: 0.8217\n",
      "Epoch 36/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3168 - auc: 0.8227\n",
      "Epoch 37/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3164 - auc: 0.8233\n",
      "Epoch 38/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3164 - auc: 0.8231\n",
      "Epoch 39/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3168 - auc: 0.8225\n",
      "Epoch 40/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3164 - auc: 0.8232\n",
      "Epoch 41/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3163 - auc: 0.8234\n",
      "Epoch 42/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3158 - auc: 0.8241\n",
      "Epoch 43/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3161 - auc: 0.8234\n",
      "Epoch 44/150\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3161 - auc: 0.8234\n",
      "Epoch 45/150\n",
      "87/87 [==============================] - 1s 6ms/step - loss: 0.3157 - auc: 0.8243\n",
      "Epoch 46/150\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.3160 - auc: 0.8238\n",
      "Epoch 47/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3156 - auc: 0.8244\n",
      "Epoch 48/150\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.3159 - auc: 0.8239\n",
      "Epoch 49/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3154 - auc: 0.8245\n",
      "Epoch 50/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3155 - auc: 0.8245\n",
      "Epoch 51/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3155 - auc: 0.8246\n",
      "Epoch 52/150\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3152 - auc: 0.8249\n",
      "Epoch 53/150\n",
      "19/87 [=====>........................] - ETA: 0s - loss: 0.3123 - auc: 0.8242"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:/Sajjad/08-2023/Python Code/Introduction to Machine Learning/IML - Unit 22.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/d%3A/Sajjad/08-2023/Python%20Code/Introduction%20to%20Machine%20Learning/IML%20-%20Unit%2022.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sgd_optimizer \u001b[39m=\u001b[39m SGD(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/d%3A/Sajjad/08-2023/Python%20Code/Introduction%20to%20Machine%20Learning/IML%20-%20Unit%2022.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mAUC\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m#change optimizer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/d%3A/Sajjad/08-2023/Python%20Code/Introduction%20to%20Machine%20Learning/IML%20-%20Unit%2022.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(trainX, trainy, epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=30, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with weights regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, epochs=150, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with early stopping with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX, trainX3, valy, trainy3 = train_test_split(trainX, trainy, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 50ms/step - loss: 0.6918 - auc: 0.5010 - val_loss: 0.6406 - val_auc: 0.5074\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.6111 - auc: 0.5310 - val_loss: 0.5592 - val_auc: 0.5603\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5373 - auc: 0.5746 - val_loss: 0.4736 - val_auc: 0.6553\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4689 - auc: 0.6368 - val_loss: 0.4055 - val_auc: 0.7288\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4193 - auc: 0.6867 - val_loss: 0.3662 - val_auc: 0.7631\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3952 - auc: 0.7112 - val_loss: 0.3477 - val_auc: 0.7794\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3790 - auc: 0.7333 - val_loss: 0.3389 - val_auc: 0.7890\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3719 - auc: 0.7442 - val_loss: 0.3341 - val_auc: 0.7952\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3657 - auc: 0.7536 - val_loss: 0.3313 - val_auc: 0.7999\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3636 - auc: 0.7573 - val_loss: 0.3294 - val_auc: 0.8032\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3578 - auc: 0.7662 - val_loss: 0.3281 - val_auc: 0.8055\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3552 - auc: 0.7713 - val_loss: 0.3271 - val_auc: 0.8075\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3535 - auc: 0.7739 - val_loss: 0.3263 - val_auc: 0.8093\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3511 - auc: 0.7767 - val_loss: 0.3254 - val_auc: 0.8106\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3484 - auc: 0.7818 - val_loss: 0.3248 - val_auc: 0.8119\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3481 - auc: 0.7817 - val_loss: 0.3243 - val_auc: 0.8130\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.3485 - auc: 0.7821 - val_loss: 0.3238 - val_auc: 0.8142\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3447 - auc: 0.7893 - val_loss: 0.3232 - val_auc: 0.8149\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3456 - auc: 0.7876 - val_loss: 0.3228 - val_auc: 0.8156\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3442 - auc: 0.7893 - val_loss: 0.3223 - val_auc: 0.8162\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3417 - auc: 0.7930 - val_loss: 0.3221 - val_auc: 0.8167\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3428 - auc: 0.7906 - val_loss: 0.3217 - val_auc: 0.8173\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3424 - auc: 0.7921 - val_loss: 0.3215 - val_auc: 0.8178\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 29ms/step - loss: 0.3405 - auc: 0.7950 - val_loss: 0.3211 - val_auc: 0.8183\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3405 - auc: 0.7936 - val_loss: 0.3208 - val_auc: 0.8187\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3398 - auc: 0.7965 - val_loss: 0.3205 - val_auc: 0.8191\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3372 - auc: 0.8002 - val_loss: 0.3202 - val_auc: 0.8194\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 29ms/step - loss: 0.3400 - auc: 0.7964 - val_loss: 0.3200 - val_auc: 0.8198\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3384 - auc: 0.7982 - val_loss: 0.3197 - val_auc: 0.8201\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3360 - auc: 0.8005 - val_loss: 0.3195 - val_auc: 0.8204\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.3382 - auc: 0.7981 - val_loss: 0.3194 - val_auc: 0.8207\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3364 - auc: 0.8002 - val_loss: 0.3191 - val_auc: 0.8209\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3346 - auc: 0.8033 - val_loss: 0.3189 - val_auc: 0.8211\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3357 - auc: 0.8015 - val_loss: 0.3187 - val_auc: 0.8214\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3367 - auc: 0.8000 - val_loss: 0.3185 - val_auc: 0.8216\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3363 - auc: 0.8012 - val_loss: 0.3183 - val_auc: 0.8217\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3337 - auc: 0.8046 - val_loss: 0.3180 - val_auc: 0.8220\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3334 - auc: 0.8032 - val_loss: 0.3180 - val_auc: 0.8222\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3330 - auc: 0.8054 - val_loss: 0.3178 - val_auc: 0.8223\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3328 - auc: 0.8048 - val_loss: 0.3176 - val_auc: 0.8225\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3314 - auc: 0.8068 - val_loss: 0.3175 - val_auc: 0.8226\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3322 - auc: 0.8064 - val_loss: 0.3174 - val_auc: 0.8228\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3317 - auc: 0.8063 - val_loss: 0.3174 - val_auc: 0.8230\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3302 - auc: 0.8090 - val_loss: 0.3172 - val_auc: 0.8231\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3314 - auc: 0.8068 - val_loss: 0.3171 - val_auc: 0.8233\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3295 - auc: 0.8097 - val_loss: 0.3169 - val_auc: 0.8235\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.3308 - auc: 0.8075 - val_loss: 0.3168 - val_auc: 0.8235\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3299 - auc: 0.8089 - val_loss: 0.3167 - val_auc: 0.8236\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.3297 - auc: 0.8095 - val_loss: 0.3165 - val_auc: 0.8237\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3300 - auc: 0.8081 - val_loss: 0.3165 - val_auc: 0.8238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f657c89f790>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX3, trainy3, validation_data=(valX, valy), epochs=50, batch_size=2048, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model (example architecture with dropout)\n",
    "model.add(Dense(20, input_dim=21, activation='relu', kernel_initializer=he_normal()))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(10, input_dim=21, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Example dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Create an SGD optimizer with momentum\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC']) #change optimizer\n",
    "model.fit(trainX, trainy, validation_data=(valX, valy), epochs=50, batch_size=2048, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e9236c168fd9c5a2497735f30867c8b2a4981b493523b82ffb7e802a066bea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
