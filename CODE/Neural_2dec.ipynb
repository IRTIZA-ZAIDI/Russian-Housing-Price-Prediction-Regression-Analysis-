{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\IML ASSIGNMENT 2\\CODE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'price_doc']\n",
    "y = df[['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Irtiza\\AppData\\Local\\Temp\\ipykernel_1032\\2966562952.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = label.fit_transform(X[col].astype(str))\n"
     ]
    }
   ],
   "source": [
    "label = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    X[col] = label.fit_transform(X[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.log1p(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178661, 271)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\IML ASSIGNMENT 2\\CODE\\Neural_2dec.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IML%20ASSIGNMENT%202/CODE/Neural_2dec.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_cols \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "num_cols = X.select_dtypes(include=['int64','float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178661, 271)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=70)\n",
    "# X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(degree=3)\n",
    "# X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178661, 271)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['row ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    df_test[col] = label_test.fit_transform(df_test[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 271)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = np.log1p(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[num_cols] = scaler.transform(df_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pca.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = poly.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 271)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all int64 to float32\n",
    "X = X.astype('float32')\n",
    "df_test = df_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import RMSprop,Adam,Adamax\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 150)               40800     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61001 (238.29 KB)\n",
      "Trainable params: 61001 (238.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = X.shape[1]\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "#kernel_regularizer=l2(0.01))\n",
    "# Add the first hidden layer with 10 neurons and specify the input shape\n",
    "model.add(Dense(150, input_dim=n_features, activation='sigmoid'))\n",
    "# Add the second hidden layer with 5 neurons\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# Add the third hidden layer with 3 neurons\n",
    "model.add(Dense(50, activation='swish'))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.add(Dense(256, input_dim=n_features,activation='relu'))\n",
    "# model.add(Dense(128, activation='sigmoid'))\n",
    "# model.add(Dense(64, activation='swish'))\n",
    "# model.add(Dense(32, activation='sigmoid'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adamax(), loss='mean_squared_error') #change optimizer\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 704986535165952.0000\n",
      "Epoch 2/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 704363697799168.0000\n",
      "Epoch 3/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 702196953907200.0000\n",
      "Epoch 4/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 697581072023552.0000\n",
      "Epoch 5/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 689621893644288.0000\n",
      "Epoch 6/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 677788856090624.0000\n",
      "Epoch 7/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 661448451686400.0000\n",
      "Epoch 8/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 641036686721024.0000\n",
      "Epoch 9/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 616980910440448.0000\n",
      "Epoch 10/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 589941071413248.0000\n",
      "Epoch 11/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 561279748014080.0000\n",
      "Epoch 12/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 531913580216320.0000\n",
      "Epoch 13/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 504547290120192.0000\n",
      "Epoch 14/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 478632262762496.0000\n",
      "Epoch 15/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 450963513016320.0000\n",
      "Epoch 16/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 420062498390016.0000\n",
      "Epoch 17/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 384918626500608.0000\n",
      "Epoch 18/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 350116942708736.0000\n",
      "Epoch 19/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 316072716664832.0000\n",
      "Epoch 20/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 282238004494336.0000\n",
      "Epoch 21/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 252025828802560.0000\n",
      "Epoch 22/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 225557673410560.0000\n",
      "Epoch 23/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 205342134763520.0000\n",
      "Epoch 24/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 190420243972096.0000\n",
      "Epoch 25/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 180224780140544.0000\n",
      "Epoch 26/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 174643872792576.0000\n",
      "Epoch 27/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 171753997336576.0000\n",
      "Epoch 28/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 170188414648320.0000\n",
      "Epoch 29/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 169098147266560.0000\n",
      "Epoch 30/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 168439406657536.0000\n",
      "Epoch 31/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 167875943858176.0000\n",
      "Epoch 32/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 167425173618688.0000\n",
      "Epoch 33/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 167040404946944.0000\n",
      "Epoch 34/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 166760007335936.0000\n",
      "Epoch 35/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 166520395137024.0000\n",
      "Epoch 36/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 166314773577728.0000\n",
      "Epoch 37/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 166120912846848.0000\n",
      "Epoch 38/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165953006469120.0000\n",
      "Epoch 39/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165789025959936.0000\n",
      "Epoch 40/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165643097735168.0000\n",
      "Epoch 41/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165509802754048.0000\n",
      "Epoch 42/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165386490216448.0000\n",
      "Epoch 43/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165273193676800.0000\n",
      "Epoch 44/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165168201859072.0000\n",
      "Epoch 45/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 165069920927744.0000\n",
      "Epoch 46/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164978401214464.0000\n",
      "Epoch 47/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164885807759360.0000\n",
      "Epoch 48/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164809840525312.0000\n",
      "Epoch 49/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164732094906368.0000\n",
      "Epoch 50/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164659097239552.0000\n",
      "Epoch 51/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164588515491840.0000\n",
      "Epoch 52/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164524896288768.0000\n",
      "Epoch 53/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164464498311168.0000\n",
      "Epoch 54/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164406130376704.0000\n",
      "Epoch 55/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164353852571648.0000\n",
      "Epoch 56/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164298252877824.0000\n",
      "Epoch 57/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164251477999616.0000\n",
      "Epoch 58/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164205676199936.0000\n",
      "Epoch 59/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164156703506432.0000\n",
      "Epoch 60/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164114542362624.0000\n",
      "Epoch 61/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164072465104896.0000\n",
      "Epoch 62/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 164035823665152.0000\n",
      "Epoch 63/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163996078440448.0000\n",
      "Epoch 64/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163956165443584.0000\n",
      "Epoch 65/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163923349209088.0000\n",
      "Epoch 66/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163890885296128.0000\n",
      "Epoch 67/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163856324231168.0000\n",
      "Epoch 68/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163825051500544.0000\n",
      "Epoch 69/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163798442835968.0000\n",
      "Epoch 70/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163765475606528.0000\n",
      "Epoch 71/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163734538420224.0000\n",
      "Epoch 72/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163709406150656.0000\n",
      "Epoch 73/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163678586404864.0000\n",
      "Epoch 74/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163656440479744.0000\n",
      "Epoch 75/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163628305088512.0000\n",
      "Epoch 76/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163603625803776.0000\n",
      "Epoch 77/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163579550498816.0000\n",
      "Epoch 78/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163556683153408.0000\n",
      "Epoch 79/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163527826341888.0000\n",
      "Epoch 80/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163510529032192.0000\n",
      "Epoch 81/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163481789661184.0000\n",
      "Epoch 82/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163461103353856.0000\n",
      "Epoch 83/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163443353059328.0000\n",
      "Epoch 84/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163418053017600.0000\n",
      "Epoch 85/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163398826328064.0000\n",
      "Epoch 86/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163375338225664.0000\n",
      "Epoch 87/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163357017505792.0000\n",
      "Epoch 88/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163333864947712.0000\n",
      "Epoch 89/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163316131430400.0000\n",
      "Epoch 90/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163296720191488.0000\n",
      "Epoch 91/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163280681172992.0000\n",
      "Epoch 92/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163258954678272.0000\n",
      "Epoch 93/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163238721355776.0000\n",
      "Epoch 94/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163221189165056.0000\n",
      "Epoch 95/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163203052994560.0000\n",
      "Epoch 96/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163185822793728.0000\n",
      "Epoch 97/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163166143119360.0000\n",
      "Epoch 98/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163143695204352.0000\n",
      "Epoch 99/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163129501679616.0000\n",
      "Epoch 100/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163111197736960.0000\n",
      "Epoch 101/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163097004212224.0000\n",
      "Epoch 102/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163079270694912.0000\n",
      "Epoch 103/200\n",
      "349/349 [==============================] - 2s 6ms/step - loss: 163058617942016.0000\n",
      "Epoch 104/200\n",
      "349/349 [==============================] - 2s 6ms/step - loss: 163041521958912.0000\n",
      "Epoch 105/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163026640568320.0000\n",
      "Epoch 106/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 163007917195264.0000\n",
      "Epoch 107/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162992213721088.0000\n",
      "Epoch 108/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162974480203776.0000\n",
      "Epoch 109/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162959665922048.0000\n",
      "Epoch 110/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162945807941632.0000\n",
      "Epoch 111/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162926329593856.0000\n",
      "Epoch 112/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162910374461440.0000\n",
      "Epoch 113/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162894905868288.0000\n",
      "Epoch 114/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162877960880128.0000\n",
      "Epoch 115/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162863448588288.0000\n",
      "Epoch 116/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162846184833024.0000\n",
      "Epoch 117/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162831890644992.0000\n",
      "Epoch 118/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162817009254400.0000\n",
      "Epoch 119/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162804325679104.0000\n",
      "Epoch 120/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162784293683200.0000\n",
      "Epoch 121/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162768908976128.0000\n",
      "Epoch 122/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162755873079296.0000\n",
      "Epoch 123/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162736830939136.0000\n",
      "Epoch 124/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162724348690432.0000\n",
      "Epoch 125/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162709802844160.0000\n",
      "Epoch 126/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162696129413120.0000\n",
      "Epoch 127/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162680744706048.0000\n",
      "Epoch 128/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162667289378816.0000\n",
      "Epoch 129/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162649975291904.0000\n",
      "Epoch 130/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162639942516736.0000\n",
      "Epoch 131/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162621873455104.0000\n",
      "Epoch 132/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162610448171008.0000\n",
      "Epoch 133/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162596841848832.0000\n",
      "Epoch 134/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162580668612608.0000\n",
      "Epoch 135/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162566525419520.0000\n",
      "Epoch 136/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162553724403712.0000\n",
      "Epoch 137/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162541829357568.0000\n",
      "Epoch 138/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162524565602304.0000\n",
      "Epoch 139/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162512469229568.0000\n",
      "Epoch 140/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162497252294656.0000\n",
      "Epoch 141/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162485978005504.0000\n",
      "Epoch 142/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162471214055424.0000\n",
      "Epoch 143/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162459654553600.0000\n",
      "Epoch 144/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162444487950336.0000\n",
      "Epoch 145/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162430579638272.0000\n",
      "Epoch 146/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162415228485632.0000\n",
      "Epoch 147/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162404222631936.0000\n",
      "Epoch 148/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162388015841280.0000\n",
      "Epoch 149/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162378134061056.0000\n",
      "Epoch 150/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162362598359040.0000\n",
      "Epoch 151/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162348153176064.0000\n",
      "Epoch 152/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162336526565376.0000\n",
      "Epoch 153/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162320739205120.0000\n",
      "Epoch 154/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162306696675328.0000\n",
      "Epoch 155/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162296059920384.0000\n",
      "Epoch 156/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162282034167808.0000\n",
      "Epoch 157/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162266397802496.0000\n",
      "Epoch 158/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162257304551424.0000\n",
      "Epoch 159/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162244134436864.0000\n",
      "Epoch 160/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162233380241408.0000\n",
      "Epoch 161/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162218851172352.0000\n",
      "Epoch 162/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162201805520896.0000\n",
      "Epoch 163/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162191974072320.0000\n",
      "Epoch 164/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162178468413440.0000\n",
      "Epoch 165/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162164274888704.0000\n",
      "Epoch 166/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162149896814592.0000\n",
      "Epoch 167/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162138068877312.0000\n",
      "Epoch 168/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162123791466496.0000\n",
      "Epoch 169/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162113590919168.0000\n",
      "Epoch 170/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162100823457792.0000\n",
      "Epoch 171/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162082586624000.0000\n",
      "Epoch 172/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162070993567744.0000\n",
      "Epoch 173/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162059769610240.0000\n",
      "Epoch 174/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162043764146176.0000\n",
      "Epoch 175/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162033731371008.0000\n",
      "Epoch 176/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162017474248704.0000\n",
      "Epoch 177/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 162007911235584.0000\n",
      "Epoch 178/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161999203860480.0000\n",
      "Epoch 179/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161982627971072.0000\n",
      "Epoch 180/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161965984972800.0000\n",
      "Epoch 181/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161957311152128.0000\n",
      "Epoch 182/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161939124649984.0000\n",
      "Epoch 183/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161929075097600.0000\n",
      "Epoch 184/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161910234284032.0000\n",
      "Epoch 185/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161905385668608.0000\n",
      "Epoch 186/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161889715748864.0000\n",
      "Epoch 187/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161875320897536.0000\n",
      "Epoch 188/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161860959600640.0000\n",
      "Epoch 189/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161852084453376.0000\n",
      "Epoch 190/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161836901072896.0000\n",
      "Epoch 191/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161824754368512.0000\n",
      "Epoch 192/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161809721982976.0000\n",
      "Epoch 193/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161797541724160.0000\n",
      "Epoch 194/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161785344688128.0000\n",
      "Epoch 195/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161772392677376.0000\n",
      "Epoch 196/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161759910428672.0000\n",
      "Epoch 197/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161747562397696.0000\n",
      "Epoch 198/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161731791814656.0000\n",
      "Epoch 199/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161726641209344.0000\n",
      "Epoch 200/200\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 161707901059072.0000\n",
      "2431/2431 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X, y, epochs=200, batch_size=512, callbacks=[early_stopping])\n",
    "y_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675/1675 [==============================] - 2s 1ms/step\n",
      "root_mean_squared_error:  12736915.956930775\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "print(\"root_mean_squared_error: \",mean_squared_error(y_test,test_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'neural_2dec_2.csv'\n",
    "with open(filepath, mode='w', newline='') as file: \n",
    "    writer = csv.writer(file)\n",
    "    c=1\n",
    "    writer.writerow(['row ID', 'price_doc'])\n",
    "    for i in y_pred:\n",
    "        writer.writerow([c ,i[0]])\n",
    "        c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 2573327.934773121\n",
    "11256709.760698644\n",
    "root_mean_squared_error:  12736915.956930775"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
